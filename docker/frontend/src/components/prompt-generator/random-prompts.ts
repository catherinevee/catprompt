
import { RandomPrompt } from './types';

export const randomPrompts: RandomPrompt[] = [
  {
    projectRequirements: "Project Overview: Building a comprehensive cloud security audit platform that automatically scans AWS infrastructure for misconfigurations and compliance violations. The platform addresses the critical need for continuous security monitoring in multi-account AWS environments where manual audits are time-consuming and prone to human error.\n\nTech Stack: Python (Django/FastAPI), React.js frontend, PostgreSQL database, Redis for caching, AWS SDK for service integration, Terraform for infrastructure provisioning, Docker for containerization, GitHub Actions for CI/CD.\n\nKey Requirements: Multi-account AWS scanning capability, automated misconfiguration detection, compliance reporting (SOC2/ISO27001), remediation guidance, real-time alerting, role-based access control, audit trail logging, and integration with existing SIEM systems.\n\nConstraints: Must support AWS GovCloud regions, handle rate limiting gracefully, maintain scanning performance under 30 minutes for large environments, ensure data encryption in transit and at rest, support air-gapped deployment scenarios.\n\nCoding Style: Follow PEP 8 for Python, use type hints throughout, implement comprehensive error handling, maintain 80%+ test coverage, use async/await patterns for I/O operations, follow REST API design principles.",
    contextTracking: "Use a CSV file to track audit progress across different AWS services (EC2, S3, IAM, VPC), including scan completion status, findings count, risk levels, and remediation progress. Update the file after each service scan and remediation action.",
    relationalContext: "You are a cloud security expert with deep knowledge of AWS, GCP, and Azure security best practices. You understand infrastructure as code, identity management, and zero-trust architectures.",
    specificTask: "Design and implement a comprehensive cloud security audit checklist for AWS resources that identifies misconfigurations, overprivileged access, and compliance violations across EC2, S3, IAM, and VPC services.",
    guidelines: "Follow these cloud security principles:\n- Apply principle of least privilege\n- Implement defense in depth\n- Use infrastructure as code for consistency\n- Enable comprehensive logging and monitoring\n- Regular security assessments and penetration testing\n- Zero-trust network architecture",
    taskConstraints: "The audit must:\n- Cover all major AWS services\n- Be automatable via scripts or tools\n- Include remediation steps for each finding\n- Generate compliance reports for SOC2/ISO27001\n- Complete execution within 30 minutes",
    finopsConsiderations: "Cost Estimation: Estimate monthly costs for audit infrastructure including Lambda functions, CloudWatch logs, and S3 storage. Cost Drivers: Primary costs from CloudTrail logging, security scanning tools, and compliance reporting storage. Right-Sizing: Use appropriately sized Lambda functions and schedule scans during off-peak hours. Cost Monitoring: Implement cost alerts for scanning infrastructure and track cost per security finding discovered. Budget Allocation: Ensure audit costs align with security budget and demonstrate ROI through prevented security incidents."
  },
  {
    projectRequirements: "Project Overview: Developing a multi-cluster Kubernetes management platform that provides centralized monitoring, deployment automation, and resource optimization across development, staging, and production environments. The platform addresses the complexity of managing multiple Kubernetes clusters with consistent policies and governance.\n\nTech Stack: Go for backend services, React.js dashboard, Kubernetes API integration, Helm for package management, Prometheus/Grafana for monitoring, ArgoCD for GitOps, etcd for cluster state management, Istio service mesh for traffic management.\n\nKey Requirements: Multi-cluster deployment orchestration, automated rollback capabilities, resource quota management, namespace isolation, RBAC policy enforcement, health monitoring, cost allocation tracking, and integration with existing CI/CD pipelines.\n\nConstraints: Support for hybrid cloud deployments, maintain cluster performance during scaling operations, ensure zero-downtime deployments, handle network partitions gracefully, support air-gapped environments, comply with regulatory requirements for data sovereignty.\n\nCoding Style: Follow Go best practices, use structured logging, implement graceful error handling, maintain comprehensive test coverage, use context for request scoping, follow Kubernetes controller patterns.",
    contextTracking: "Use a YAML configuration file to track cluster states, deployment statuses, resource utilization metrics, and policy compliance across all managed clusters. Update tracking after each deployment, scaling operation, and policy change.",
    relationalContext: "You are a Kubernetes platform engineer with expertise in container orchestration, microservices architecture, and cloud-native technologies. You understand distributed systems, service mesh patterns, and GitOps workflows.",
    specificTask: "Design and implement a comprehensive Kubernetes cluster management solution that automates deployments, monitors cluster health, and enforces security policies across multiple environments.",
    guidelines: "Follow these Kubernetes best practices:\n- Use declarative configuration management\n- Implement proper resource requests and limits\n- Apply security contexts and pod security policies\n- Use namespaces for logical separation\n- Implement proper secrets management\n- Follow the principle of least privilege for RBAC",
    taskConstraints: "The solution must:\n- Support multiple Kubernetes distributions\n- Handle cluster failures gracefully\n- Provide real-time monitoring and alerting\n- Maintain audit logs for compliance\n- Complete deployments within 10 minutes",
    finopsConsiderations: "Cost Estimation: Calculate monthly costs for cluster nodes, load balancers, and persistent storage across environments. Cost Drivers: Primary costs from compute resources, network traffic, and storage volumes. Right-Sizing: Implement horizontal pod autoscaling and cluster autoscaling. Cost Monitoring: Track resource usage per namespace and implement chargeback mechanisms. Budget Allocation: Optimize costs through spot instances and reserved capacity planning."
  },
  {
    projectRequirements: "Project Overview: Building an enterprise-grade infrastructure automation platform using Terraform that manages multi-cloud deployments across AWS, Azure, and GCP. The platform standardizes infrastructure provisioning, enforces compliance policies, and provides cost optimization recommendations.\n\nTech Stack: Terraform Enterprise, Go for custom providers, Python for automation scripts, Jenkins for CI/CD, Vault for secrets management, Consul for service discovery, Sentinel for policy enforcement, Terraform Cloud for state management.\n\nKey Requirements: Multi-cloud resource provisioning, infrastructure drift detection, automated compliance scanning, cost estimation and optimization, module versioning and registry, approval workflows, disaster recovery automation, and integration with existing ITSM systems.\n\nConstraints: Support for enterprise networking requirements, maintain state consistency across teams, ensure secure credential management, handle provider API rate limits, support air-gapped deployments, comply with SOX and PCI DSS requirements.\n\nCoding Style: Follow Terraform best practices, use consistent naming conventions, implement proper module structure, maintain comprehensive documentation, use semantic versioning, follow HCL style guidelines.",
    contextTracking: "Use a JSON file to track infrastructure state changes, deployment history, compliance violations, cost impacts, and approval statuses across all cloud environments. Update tracking after each Terraform apply operation.",
    relationalContext: "You are a DevOps infrastructure architect with deep expertise in infrastructure as code, multi-cloud strategies, and enterprise automation. You understand compliance requirements, security best practices, and cost optimization techniques.",
    specificTask: "Design and implement a comprehensive Terraform-based infrastructure automation framework that provisions and manages resources across multiple cloud providers with built-in governance and cost controls.",
    guidelines: "Follow these Terraform best practices:\n- Use remote state with proper locking\n- Implement modular and reusable code\n- Use workspaces for environment separation\n- Implement proper variable validation\n- Use data sources instead of hardcoded values\n- Follow the principle of immutable infrastructure",
    taskConstraints: "The framework must:\n- Support all major cloud providers\n- Include automated testing and validation\n- Provide rollback capabilities\n- Generate compliance reports\n- Complete provisioning within 20 minutes",
    finopsConsiderations: "Cost Estimation: Estimate infrastructure costs before deployment using Terraform plan output and cloud pricing APIs. Cost Drivers: Primary costs from compute instances, storage, and network resources. Right-Sizing: Implement automated resource sizing recommendations based on utilization metrics. Cost Monitoring: Track cost trends and implement budget alerts. Budget Allocation: Enforce cost limits through Sentinel policies and approval workflows."
  },
  {
    projectRequirements: "Project Overview: Developing a high-performance data processing pipeline using Python that handles real-time analytics for e-commerce transactions. The system processes millions of events per day, provides fraud detection, and generates business intelligence reports.\n\nTech Stack: Python (FastAPI, Celery, Pandas, NumPy), Apache Kafka for streaming, Redis for caching, PostgreSQL for transactional data, ClickHouse for analytics, Apache Airflow for workflow orchestration, Docker for containerization, Kubernetes for orchestration.\n\nKey Requirements: Real-time event processing, machine learning model integration, fraud detection algorithms, data quality validation, scalable architecture, API rate limiting, monitoring and alerting, data retention policies, and integration with existing business systems.\n\nConstraints: Handle peak loads of 100,000 events per minute, maintain sub-second processing latency, ensure data consistency and accuracy, support multi-region deployment, comply with PCI DSS and GDPR requirements, maintain 99.9% uptime.\n\nCoding Style: Follow PEP 8 guidelines, use type hints throughout, implement async/await patterns, use dataclasses for structured data, maintain comprehensive logging, follow SOLID principles, implement proper exception handling.",
    contextTracking: "Use a PostgreSQL database to track processing metrics, error rates, model performance, data quality scores, and system health across all pipeline components. Update metrics in real-time during event processing.",
    relationalContext: "You are a senior Python developer with expertise in distributed systems, data engineering, and machine learning. You understand event-driven architectures, stream processing, and performance optimization techniques.",
    specificTask: "Design and implement a scalable Python-based data processing pipeline that handles real-time e-commerce events with integrated fraud detection and analytics capabilities.",
    guidelines: "Follow these Python best practices:\n- Use virtual environments for dependency isolation\n- Implement proper error handling and logging\n- Use asyncio for concurrent operations\n- Apply design patterns appropriately\n- Write comprehensive unit and integration tests\n- Use configuration management for environment settings",
    taskConstraints: "The pipeline must:\n- Process events with sub-second latency\n- Scale horizontally under load\n- Maintain data accuracy and consistency\n- Provide real-time monitoring\n- Support graceful degradation",
    finopsConsiderations: "Cost Estimation: Calculate monthly costs for compute resources, message queuing, and data storage based on expected throughput. Cost Drivers: Primary costs from CPU-intensive processing, memory usage, and data transfer. Right-Sizing: Implement auto-scaling based on queue depth and CPU utilization. Cost Monitoring: Track processing costs per event and implement efficiency metrics. Budget Allocation: Optimize costs through reserved instances and efficient resource utilization."
  },
  {
    projectRequirements: "Project Overview: Building a containerized microservices platform using Docker that supports development, testing, and production environments. The platform provides automated builds, security scanning, and orchestration capabilities for enterprise applications.\n\nTech Stack: Docker Engine, Docker Compose, Harbor registry, Jenkins for CI/CD, Kubernetes for orchestration, Prometheus for monitoring, Grafana for visualization, Trivy for security scanning, Docker Bench for compliance.\n\nKey Requirements: Multi-stage Docker builds, automated image scanning, registry management, orchestration automation, secret management, network isolation, logging aggregation, backup and recovery, and integration with existing development workflows.\n\nConstraints: Support for legacy application containerization, maintain image size optimization, ensure security compliance, handle resource constraints, support air-gapped deployments, comply with enterprise security policies.\n\nCoding Style: Follow Docker best practices, use multi-stage builds, implement proper layer caching, maintain minimal base images, use explicit versioning, follow security hardening guidelines.",
    contextTracking: "Use a JSON file to track container builds, image vulnerabilities, deployment statuses, resource usage, and security scan results across all environments. Update tracking after each build and deployment operation.",
    relationalContext: "You are a DevOps engineer specializing in containerization technologies, microservices architecture, and container security. You understand Docker internals, registry management, and orchestration patterns.",
    specificTask: "Design and implement a comprehensive Docker-based containerization strategy that includes automated builds, security scanning, and deployment orchestration for enterprise microservices.",
    guidelines: "Follow these Docker best practices:\n- Use official base images when possible\n- Implement multi-stage builds for optimization\n- Run containers as non-root users\n- Use specific image tags, not 'latest'\n- Implement proper health checks\n- Use .dockerignore for build optimization",
    taskConstraints: "The platform must:\n- Support multiple programming languages\n- Include automated vulnerability scanning\n- Provide deployment rollback capabilities\n- Maintain image size under 1GB\n- Complete builds within 5 minutes",
    finopsConsiderations: "Cost Estimation: Calculate monthly costs for container registry storage, compute resources, and build infrastructure. Cost Drivers: Primary costs from registry storage, build minutes, and runtime resources. Right-Sizing: Optimize container resource allocation and implement image cleanup policies. Cost Monitoring: Track storage usage and build costs per application. Budget Allocation: Implement retention policies and optimize build frequency."
  },
  {
    projectRequirements: "Project Overview: Developing a comprehensive AWS cloud architecture for a financial services company that includes multi-account setup, compliance automation, and disaster recovery capabilities. The platform ensures regulatory compliance while maintaining high performance and availability.\n\nTech Stack: AWS Organizations, AWS Control Tower, AWS Config, CloudFormation, Lambda functions, API Gateway, RDS Aurora, ElastiCache, CloudFront, AWS WAF, GuardDuty, SecurityHub, CloudTrail, CloudWatch.\n\nKey Requirements: Multi-account governance, automated compliance monitoring, data encryption at rest and in transit, backup and disaster recovery, network segmentation, identity and access management, cost optimization, audit logging, and integration with on-premises systems.\n\nConstraints: Comply with SOX, PCI DSS, and SOC 2 requirements, maintain 99.99% uptime, handle peak loads of 50,000 requests per minute, ensure data sovereignty, support global deployment, maintain sub-100ms response times.\n\nCoding Style: Follow AWS Well-Architected Framework principles, use Infrastructure as Code, implement proper tagging strategies, use AWS native services when possible, follow security by design principles.",
    contextTracking: "Use AWS Config and CloudTrail to track resource changes, compliance status, cost allocation, security findings, and operational metrics across all AWS accounts. Generate monthly compliance and cost reports.",
    relationalContext: "You are an AWS Solutions Architect with expertise in financial services, regulatory compliance, and enterprise cloud migrations. You understand AWS services, security best practices, and cost optimization strategies.",
    specificTask: "Design and implement a comprehensive AWS cloud architecture that meets financial services regulatory requirements while providing scalability, security, and cost optimization.",
    guidelines: "Follow these AWS best practices:\n- Implement the principle of least privilege\n- Use AWS Organizations for account management\n- Apply security groups and NACLs properly\n- Implement proper backup and disaster recovery\n- Use CloudFormation for infrastructure provisioning\n- Enable comprehensive logging and monitoring",
    taskConstraints: "The architecture must:\n- Pass all compliance audits\n- Support global deployment\n- Provide automated failover\n- Maintain cost efficiency\n- Enable rapid scaling",
    finopsConsiderations: "Cost Estimation: Estimate monthly AWS costs including compute, storage, data transfer, and managed services based on expected usage. Cost Drivers: Primary costs from EC2 instances, RDS databases, and data transfer. Right-Sizing: Implement auto-scaling and use appropriate instance types. Cost Monitoring: Use AWS Cost Explorer and implement cost allocation tags. Budget Allocation: Set up billing alerts and implement cost controls through IAM policies."
  },
  {
    projectRequirements: "Project Overview: Building a comprehensive Azure cloud infrastructure for a healthcare organization that includes secure data processing, AI/ML capabilities, and compliance with healthcare regulations. The platform handles patient data processing and medical image analysis.\n\nTech Stack: Azure Resource Manager, Azure Active Directory, Azure Virtual Network, Azure SQL Database, Azure Cosmos DB, Azure Machine Learning, Azure Cognitive Services, Azure Key Vault, Azure Monitor, Azure Security Center, Azure Backup.\n\nKey Requirements: HIPAA compliance automation, secure data processing pipelines, AI-powered medical image analysis, patient data anonymization, disaster recovery, network isolation, identity federation, audit logging, and integration with existing healthcare systems.\n\nConstraints: Comply with HIPAA, HITECH, and FDA regulations, maintain 99.9% uptime, handle sensitive patient data securely, ensure data residency requirements, support real-time processing, maintain audit trails for 7 years.\n\nCoding Style: Follow Azure Well-Architected Framework, use ARM templates and Bicep, implement Azure Policy for governance, use managed identities, follow zero-trust security model.",
    contextTracking: "Use Azure Monitor and Log Analytics to track resource utilization, compliance status, security events, AI model performance, and patient data access across all Azure services. Generate automated compliance reports.",
    relationalContext: "You are an Azure Solutions Architect specializing in healthcare technology, regulatory compliance, and AI/ML implementations. You understand Azure services, healthcare data standards, and privacy regulations.",
    specificTask: "Design and implement a comprehensive Azure cloud architecture for healthcare data processing that includes AI/ML capabilities while maintaining strict compliance with healthcare regulations.",
    guidelines: "Follow these Azure best practices:\n- Implement Azure Policy for compliance automation\n- Use Azure Key Vault for secrets management\n- Apply network security groups and Azure Firewall\n- Implement proper backup and disaster recovery\n- Use Azure AD for identity management\n- Enable comprehensive monitoring and alerting",
    taskConstraints: "The architecture must:\n- Pass HIPAA compliance audits\n- Process medical images within 30 seconds\n- Maintain data encryption throughout\n- Support automated failover\n- Enable real-time monitoring",
    finopsConsiderations: "Cost Estimation: Calculate monthly Azure costs including compute, storage, AI services, and networking based on expected patient data volume. Cost Drivers: Primary costs from virtual machines, AI services, and storage. Right-Sizing: Implement auto-scaling for compute resources and optimize AI service usage. Cost Monitoring: Use Azure Cost Management and implement cost allocation tags. Budget Allocation: Set up spending limits and implement cost optimization policies."
  },
  {
    projectRequirements: "Project Overview: Developing a comprehensive Linux administration automation platform that manages hundreds of servers across multiple data centers. The platform provides configuration management, security hardening, patch management, and performance monitoring.\n\nTech Stack: Ansible for configuration management, Terraform for infrastructure provisioning, Prometheus and Grafana for monitoring, ELK stack for log aggregation, RHEL/Ubuntu for operating systems, Puppet for configuration drift detection, Nagios for alerting.\n\nKey Requirements: Automated patch management, security compliance enforcement, configuration standardization, performance optimization, backup automation, disaster recovery, user access management, and integration with existing ITSM systems.\n\nConstraints: Support for heterogeneous Linux distributions, maintain zero-downtime patching, ensure security compliance, handle legacy system integration, support air-gapped environments, comply with enterprise security policies.\n\nCoding Style: Follow shell scripting best practices, use Ansible playbooks, implement proper error handling, maintain idempotent operations, use version control for configurations, follow security hardening guidelines.",
    contextTracking: "Use a centralized database to track server configurations, patch levels, security compliance status, performance metrics, and incident history across all managed Linux systems. Update tracking after each automation run.",
    relationalContext: "You are a Linux system administrator with expertise in automation, security hardening, and enterprise infrastructure management. You understand Linux internals, networking, and security best practices.",
    specificTask: "Design and implement a comprehensive Linux administration automation framework that manages server configurations, security compliance, and performance optimization across enterprise infrastructure.",
    guidelines: "Follow these Linux administration best practices:\n- Implement configuration management with version control\n- Use sudo for privilege escalation\n- Apply security hardening standards (CIS benchmarks)\n- Implement proper backup and recovery procedures\n- Use automation for repetitive tasks\n- Maintain comprehensive documentation",
    taskConstraints: "The automation must:\n- Support multiple Linux distributions\n- Provide zero-downtime patching\n- Maintain security compliance\n- Complete operations within maintenance windows\n- Provide rollback capabilities",
    finopsConsiderations: "Cost Estimation: Calculate monthly costs for server hardware, software licenses, and management tools based on server count. Cost Drivers: Primary costs from hardware, enterprise support, and management software. Right-Sizing: Optimize server utilization and implement power management. Cost Monitoring: Track resource usage and implement capacity planning. Budget Allocation: Optimize licensing costs and implement server consolidation strategies."
  },
  {
    projectRequirements: "Project Overview: Building a comprehensive CloudFormation template library for AWS infrastructure that provides standardized, reusable, and compliant infrastructure components. The library supports rapid deployment of secure and cost-optimized AWS resources.\n\nTech Stack: AWS CloudFormation, AWS CLI, Python for automation scripts, AWS Config for compliance, AWS Service Catalog for governance, GitHub for version control, Jenkins for CI/CD, AWS CodePipeline for deployment automation.\n\nKey Requirements: Modular template design, parameter validation, cross-stack references, automated testing, security compliance, cost optimization, documentation generation, version management, and integration with existing DevOps workflows.\n\nConstraints: Support for all AWS regions, maintain template compatibility across AWS updates, ensure security best practices, handle resource dependencies, support nested stacks, comply with enterprise governance policies.\n\nCoding Style: Follow CloudFormation best practices, use consistent naming conventions, implement proper parameter validation, maintain comprehensive documentation, use semantic versioning, follow JSON/YAML formatting standards.",
    contextTracking: "Use AWS Config and CloudTrail to track template deployments, resource configurations, compliance status, cost impacts, and usage patterns across all CloudFormation stacks. Generate deployment and compliance reports.",
    relationalContext: "You are an AWS DevOps engineer specializing in Infrastructure as Code, CloudFormation automation, and enterprise cloud governance. You understand AWS services, template design patterns, and compliance requirements.",
    specificTask: "Design and implement a comprehensive CloudFormation template library that provides standardized, secure, and cost-optimized AWS infrastructure components with automated testing and deployment capabilities.",
    guidelines: "Follow these CloudFormation best practices:\n- Use nested stacks for modularity\n- Implement proper parameter validation\n- Use intrinsic functions effectively\n- Apply consistent resource naming\n- Implement proper error handling\n- Use cross-stack references appropriately",
    taskConstraints: "The template library must:\n- Support all major AWS services\n- Include automated testing\n- Provide rollback capabilities\n- Maintain backward compatibility\n- Complete deployments within 15 minutes",
    finopsConsiderations: "Cost Estimation: Estimate AWS resource costs using CloudFormation cost estimation features and AWS pricing APIs. Cost Drivers: Primary costs from compute instances, storage, and managed services. Right-Sizing: Implement cost optimization recommendations in templates. Cost Monitoring: Track costs by stack and implement cost allocation tags. Budget Allocation: Include cost controls and budget alerts in template designs."
  },
  {
    projectRequirements: "Project Overview: Developing a comprehensive ARM template library for Azure infrastructure that provides standardized, enterprise-grade resource deployments. The platform ensures security compliance, cost optimization, and operational excellence across Azure environments.\n\nTech Stack: Azure Resource Manager, Azure PowerShell, Azure CLI, Azure DevOps for CI/CD, Azure Policy for governance, Azure Monitor for monitoring, Azure Key Vault for secrets, GitHub for version control, Pester for testing.\n\nKey Requirements: Modular template design, parameter validation, linked template support, automated testing, security baseline enforcement, cost optimization, resource tagging, disaster recovery configuration, and integration with Azure DevOps pipelines.\n\nConstraints: Support for all Azure regions, maintain template compatibility across Azure updates, ensure security compliance, handle complex resource dependencies, support template composition, comply with enterprise governance requirements.\n\nCoding Style: Follow ARM template best practices, use consistent naming conventions, implement proper parameter validation, maintain comprehensive documentation, use semantic versioning, follow JSON formatting standards.",
    contextTracking: "Use Azure Monitor and Activity Log to track template deployments, resource configurations, compliance status, cost impacts, and performance metrics across all ARM template deployments. Generate automated governance reports.",
    relationalContext: "You are an Azure DevOps engineer specializing in Infrastructure as Code, ARM template development, and enterprise Azure governance. You understand Azure services, template design patterns, and compliance frameworks.",
    specificTask: "Design and implement a comprehensive ARM template library that provides standardized, secure, and cost-optimized Azure infrastructure components with automated testing and governance capabilities.",
    guidelines: "Follow these ARM template best practices:\n- Use linked templates for modularity\n- Implement parameter validation with allowed values\n- Use variables for complex expressions\n- Apply consistent resource naming conventions\n- Implement proper dependency management\n- Use outputs for cross-template communication",
    taskConstraints: "The template library must:\n- Support all major Azure services\n- Include automated validation\n- Provide deployment rollback\n- Maintain version compatibility\n- Complete deployments within 20 minutes",
    finopsConsiderations: "Cost Estimation: Estimate Azure resource costs using Azure pricing calculator and cost management APIs. Cost Drivers: Primary costs from virtual machines, storage accounts, and managed services. Right-Sizing: Include VM sizing recommendations and auto-scaling configurations. Cost Monitoring: Implement cost tracking tags and budget alerts. Budget Allocation: Include cost optimization policies and spending limits in templates."
  },
  {
    projectRequirements: "Project Overview: Building a comprehensive Bicep template ecosystem for Azure infrastructure that provides modern, type-safe, and maintainable infrastructure deployments. The platform leverages Bicep's advanced features for enterprise-scale Azure resource management.\n\nTech Stack: Azure Bicep, Azure CLI, Azure PowerShell, Azure DevOps for CI/CD, Azure Resource Manager, Azure Policy, Azure Monitor, GitHub Actions for automation, Visual Studio Code with Bicep extension.\n\nKey Requirements: Modular Bicep template design, strong typing and validation, module registry integration, automated testing with Pester, security baseline enforcement, cost optimization, comprehensive documentation, version management, and integration with GitOps workflows.\n\nConstraints: Support for latest Azure features, maintain Bicep compatibility with ARM, ensure security best practices, handle complex resource relationships, support module composition, comply with enterprise governance standards.\n\nCoding Style: Follow Bicep best practices, use strong typing, implement proper parameter validation, maintain clean and readable syntax, use consistent naming conventions, follow semantic versioning for modules.",
    contextTracking: "Use Azure Monitor and deployment history to track Bicep template deployments, resource configurations, module usage, performance metrics, and compliance status across all Azure environments. Generate deployment analytics and governance reports.",
    relationalContext: "You are an Azure Infrastructure Engineer specializing in modern Infrastructure as Code, Bicep development, and enterprise Azure architecture. You understand Bicep language features, Azure services, and infrastructure automation patterns.",
    specificTask: "Design and implement a comprehensive Bicep template ecosystem that provides type-safe, modular, and maintainable Azure infrastructure components with advanced testing and governance capabilities.",
    guidelines: "Follow these Bicep best practices:\n- Use strong typing for all parameters\n- Implement modules for reusability\n- Use decorators for parameter validation\n- Apply consistent naming with conventions\n- Implement proper scope management\n- Use symbolic names for resources",
    taskConstraints: "The Bicep ecosystem must:\n- Leverage latest Bicep features\n- Include comprehensive testing\n- Support module composition\n- Provide type safety\n- Complete deployments efficiently",
    finopsConsiderations: "Cost Estimation: Integrate Azure cost estimation APIs with Bicep deployment previews to provide accurate cost forecasting. Cost Drivers: Focus on compute, storage, and networking resource optimization. Right-Sizing: Include intelligent VM sizing and auto-scaling configurations in modules. Cost Monitoring: Implement cost tracking through resource tags and budget integration. Budget Allocation: Build cost governance into Bicep modules with spending controls and optimization recommendations."
  },
  {
    projectRequirements: "Project Overview: Developing a comprehensive FinOps platform that provides cloud cost optimization, budget management, and financial governance across multi-cloud environments. The platform enables organizations to implement financial accountability and cost transparency in cloud operations.\n\nTech Stack: Python (Django/FastAPI) for backend, React.js for frontend, PostgreSQL for data storage, Apache Airflow for data pipelines, Grafana for cost visualization, AWS Cost Explorer API, Azure Cost Management API, GCP Billing API, Slack/Teams integration.\n\nKey Requirements: Multi-cloud cost aggregation, budget tracking and alerting, cost allocation and chargeback, resource optimization recommendations, showback reporting, anomaly detection, forecasting, approval workflows, and integration with existing financial systems.\n\nConstraints: Handle large volumes of billing data, ensure data accuracy and consistency, maintain real-time cost visibility, support complex organizational structures, comply with financial reporting standards, handle multi-currency scenarios.\n\nCoding Style: Follow financial software development standards, implement comprehensive audit logging, use decimal arithmetic for financial calculations, maintain data integrity, follow security best practices for financial data.",
    contextTracking: "Use a time-series database to track cost trends, budget utilization, optimization opportunities, and financial KPIs across all cloud providers and business units. Update cost tracking hourly and generate daily financial reports.",
    relationalContext: "You are a FinOps practitioner with expertise in cloud financial management, cost optimization, and enterprise financial systems. You understand cloud pricing models, accounting principles, and financial governance frameworks.",
    specificTask: "Design and implement a comprehensive FinOps platform that provides multi-cloud cost visibility, optimization recommendations, and financial governance capabilities for enterprise cloud operations.",
    guidelines: "Follow these FinOps best practices:\n- Implement the FinOps framework phases (Inform, Optimize, Operate)\n- Establish cost allocation and tagging strategies\n- Create cost transparency and accountability\n- Implement automated cost optimization\n- Enable financial and engineering collaboration\n- Focus on business value optimization",
    taskConstraints: "The platform must:\n- Process billing data from multiple clouds\n- Provide real-time cost visibility\n- Generate accurate financial reports\n- Support complex cost allocation\n- Enable automated optimization",
    finopsConsiderations: "Cost Estimation: Calculate platform operational costs including data processing, storage, and API calls across cloud providers. Cost Drivers: Primary costs from data ingestion, analytics processing, and dashboard infrastructure. Right-Sizing: Optimize data pipeline resources and implement efficient caching strategies. Cost Monitoring: Track platform costs and demonstrate ROI through cost savings achieved. Budget Allocation: Ensure platform costs are offset by realized savings and optimization benefits."
  },
  {
    projectRequirements: "Project Overview: Building an enterprise Kubernetes security and compliance platform that provides automated vulnerability scanning, policy enforcement, and security monitoring across multiple clusters. The platform addresses the critical need for comprehensive container security in production environments.\n\nTech Stack: Go for security controllers, Kubernetes API, Open Policy Agent (OPA) for policy enforcement, Falco for runtime security, Prometheus for metrics, Grafana for visualization, Harbor for image scanning, Helm for deployment, ArgoCD for GitOps.\n\nKey Requirements: Automated vulnerability scanning, policy as code implementation, runtime threat detection, compliance reporting (CIS Kubernetes Benchmark), network policy enforcement, secret management, RBAC auditing, and integration with existing security tools.\n\nConstraints: Support multiple Kubernetes distributions, maintain low performance overhead, ensure real-time threat detection, handle air-gapped deployments, comply with security frameworks (NIST, ISO 27001), support multi-tenancy.\n\nCoding Style: Follow Kubernetes controller patterns, implement proper error handling, use structured logging, maintain comprehensive test coverage, follow Go best practices, implement graceful shutdown handling.",
    contextTracking: "Use a time-series database to track security events, policy violations, vulnerability trends, compliance scores, and remediation progress across all Kubernetes clusters. Generate daily security reports and real-time alerts.",
    relationalContext: "You are a Kubernetes security engineer with expertise in container security, policy enforcement, and compliance frameworks. You understand Kubernetes security primitives, threat modeling, and security automation.",
    specificTask: "Design and implement a comprehensive Kubernetes security platform that provides automated vulnerability management, policy enforcement, and compliance monitoring for enterprise container environments.",
    guidelines: "Follow these Kubernetes security best practices:\n- Implement pod security policies/standards\n- Use network policies for micro-segmentation\n- Apply principle of least privilege for RBAC\n- Implement runtime security monitoring\n- Use admission controllers for policy enforcement\n- Enable comprehensive audit logging",
    taskConstraints: "The security platform must:\n- Scan all container images automatically\n- Enforce policies in real-time\n- Detect runtime threats within seconds\n- Generate compliance reports\n- Support multiple cluster environments",
    finopsConsiderations: "Cost Estimation: Calculate monthly costs for security scanning infrastructure, monitoring tools, and policy enforcement resources. Cost Drivers: Primary costs from vulnerability databases, scanning compute resources, and monitoring infrastructure. Right-Sizing: Optimize scanning schedules and resource allocation for security tools. Cost Monitoring: Track security tooling costs and measure ROI through prevented security incidents. Budget Allocation: Balance security investment with risk reduction and compliance benefits."
  },
  {
    projectRequirements: "Project Overview: Developing an advanced Terraform automation platform that provides self-service infrastructure provisioning, automated testing, and compliance validation across enterprise environments. The platform enables developers to provision infrastructure safely while maintaining governance and cost controls.\n\nTech Stack: Terraform Enterprise, Go for custom providers, Python for automation, Terratest for testing, Sentinel for policy enforcement, Atlantis for pull request automation, Vault for secrets, Jenkins for CI/CD, GitHub for version control.\n\nKey Requirements: Self-service portal for infrastructure requests, automated testing and validation, policy-driven governance, cost estimation and approval workflows, drift detection and remediation, module marketplace, audit logging, and integration with ITSM systems.\n\nConstraints: Support for multiple cloud providers, ensure state consistency across teams, maintain security and compliance, handle complex dependencies, support enterprise networking, comply with change management processes.\n\nCoding Style: Follow Terraform best practices, use proper module structure, implement comprehensive testing, maintain detailed documentation, use semantic versioning, follow HCL style guidelines.",
    contextTracking: "Use a comprehensive database to track infrastructure requests, approval workflows, deployment status, cost impacts, policy violations, and resource lifecycle across all Terraform operations. Generate governance and utilization reports.",
    relationalContext: "You are a Platform Engineering specialist with expertise in Terraform automation, infrastructure governance, and developer productivity. You understand infrastructure patterns, compliance requirements, and self-service platforms.",
    specificTask: "Design and implement an advanced Terraform automation platform that provides self-service infrastructure provisioning with built-in governance, testing, and cost controls for enterprise development teams.",
    guidelines: "Follow these advanced Terraform practices:\n- Implement proper state management strategies\n- Use workspace isolation for environments\n- Apply policy as code with Sentinel\n- Implement automated testing pipelines\n- Use module composition patterns\n- Enable self-service with guardrails",
    taskConstraints: "The automation platform must:\n- Support multiple cloud providers\n- Provide real-time cost estimation\n- Enforce governance policies automatically\n- Enable developer self-service\n- Maintain infrastructure compliance",
    finopsConsiderations: "Cost Estimation: Implement real-time cost estimation using cloud pricing APIs and Terraform plan output analysis. Cost Drivers: Focus on infrastructure provisioning costs, platform operational expenses, and governance tooling. Right-Sizing: Include intelligent resource sizing recommendations and cost optimization policies. Cost Monitoring: Track infrastructure costs per team and project with automated chargeback. Budget Allocation: Implement budget controls and approval workflows for cost-effective infrastructure provisioning."
  },
  {
    projectRequirements: "Project Overview: Building a high-performance Python data analytics platform that processes large-scale financial datasets for risk analysis and regulatory reporting. The platform provides real-time data processing, machine learning capabilities, and automated compliance reporting.\n\nTech Stack: Python (NumPy, Pandas, Dask, Scikit-learn), Apache Spark for distributed processing, PostgreSQL for transactional data, ClickHouse for analytics, Apache Kafka for streaming, Redis for caching, Jupyter for analysis, MLflow for model management.\n\nKey Requirements: Real-time data ingestion and processing, complex financial calculations, machine learning model deployment, regulatory reporting automation, data lineage tracking, performance optimization, horizontal scaling, and integration with trading systems.\n\nConstraints: Handle terabytes of financial data daily, maintain sub-second query response times, ensure data accuracy for regulatory compliance, support real-time risk calculations, comply with financial regulations (MiFID II, Basel III), maintain audit trails.\n\nCoding Style: Follow financial software standards, use type hints throughout, implement comprehensive error handling, maintain high test coverage, use async programming patterns, follow PEP 8 guidelines, implement proper logging.",
    contextTracking: "Use a data catalog to track data lineage, processing metrics, model performance, regulatory compliance status, and system health across all analytics pipelines. Update tracking in real-time during data processing operations.",
    relationalContext: "You are a Python data engineer specializing in financial analytics, quantitative analysis, and regulatory technology. You understand financial markets, risk management, and regulatory requirements.",
    specificTask: "Design and implement a high-performance Python analytics platform that processes large-scale financial datasets with real-time risk analysis and automated regulatory reporting capabilities.",
    guidelines: "Follow these Python analytics best practices:\n- Use vectorized operations with NumPy/Pandas\n- Implement proper memory management\n- Use distributed computing for large datasets\n- Apply proper data validation and cleaning\n- Implement reproducible analytics workflows\n- Use appropriate data structures for performance",
    taskConstraints: "The analytics platform must:\n- Process terabytes of data efficiently\n- Provide real-time risk calculations\n- Generate accurate regulatory reports\n- Support model deployment and monitoring\n- Maintain data quality and lineage",
    finopsConsiderations: "Cost Estimation: Calculate monthly costs for compute resources, data storage, and analytics infrastructure based on data volume and processing requirements. Cost Drivers: Primary costs from distributed computing, storage systems, and real-time processing. Right-Sizing: Optimize cluster sizing and implement efficient data partitioning strategies. Cost Monitoring: Track processing costs per analytics job and implement usage-based cost allocation. Budget Allocation: Balance performance requirements with cost efficiency through intelligent resource scheduling."
  },
  {
    projectRequirements: "Project Overview: Developing a comprehensive Docker containerization strategy for legacy application modernization that provides automated migration, security hardening, and operational excellence. The platform enables organizations to modernize legacy applications while maintaining reliability and security.\n\nTech Stack: Docker Engine, Docker Compose, Kubernetes for orchestration, Harbor for registry management, Trivy for security scanning, Buildah for advanced builds, Skopeo for image management, Prometheus for monitoring, Grafana for visualization.\n\nKey Requirements: Legacy application containerization, automated Dockerfile generation, security vulnerability scanning, image optimization, multi-stage builds, registry management, orchestration automation, monitoring and logging, and integration with existing CI/CD pipelines.\n\nConstraints: Support for diverse legacy technologies, maintain application compatibility, ensure security compliance, optimize image sizes, handle complex dependencies, support air-gapped environments, minimize downtime during migration.\n\nCoding Style: Follow Docker security best practices, use multi-stage builds, implement proper layer optimization, maintain minimal attack surface, use explicit base image versions, follow container security guidelines.",
    contextTracking: "Use a comprehensive database to track containerization progress, security scan results, image metrics, deployment status, and migration milestones across all legacy applications. Generate migration progress and security compliance reports.",
    relationalContext: "You are a DevOps modernization specialist with expertise in legacy application migration, Docker containerization, and application security. You understand legacy technologies, containerization challenges, and modernization strategies.",
    specificTask: "Design and implement a comprehensive Docker containerization strategy that enables secure and efficient migration of legacy applications to modern container platforms with automated tooling and governance.",
    guidelines: "Follow these Docker modernization best practices:\n- Analyze application dependencies thoroughly\n- Use appropriate base images for legacy apps\n- Implement security scanning in build pipeline\n- Optimize container layers for efficiency\n- Apply proper secrets management\n- Use health checks for container reliability",
    taskConstraints: "The containerization strategy must:\n- Support diverse legacy applications\n- Maintain application functionality\n- Ensure security compliance\n- Optimize resource utilization\n- Enable automated deployment",
    finopsConsiderations: "Cost Estimation: Calculate migration costs including containerization effort, infrastructure changes, and operational overhead. Cost Drivers: Primary costs from development effort, testing infrastructure, and new platform operations. Right-Sizing: Optimize container resource allocation and implement efficient resource sharing. Cost Monitoring: Track migration costs and measure operational savings from containerization. Budget Allocation: Balance migration investment with long-term operational efficiency and modernization benefits."
  },
  {
    projectRequirements: "Project Overview: Building an enterprise AWS Well-Architected assessment and optimization platform that automatically evaluates AWS environments against the five pillars and provides actionable improvement recommendations. The platform helps organizations maintain architectural excellence and operational efficiency.\n\nTech Stack: Python (Boto3, FastAPI), AWS Config, AWS Trusted Advisor, AWS Systems Manager, AWS Cost Explorer, AWS Security Hub, CloudFormation for remediation, Lambda for automation, DynamoDB for state management, S3 for reporting.\n\nKey Requirements: Automated Well-Architected assessments, security pillar evaluation, reliability analysis, performance optimization recommendations, cost optimization insights, operational excellence metrics, sustainability assessments, and integration with AWS native tools.\n\nConstraints: Support for complex AWS architectures, handle large-scale environments efficiently, ensure accurate assessment results, provide actionable recommendations, support multi-account organizations, comply with enterprise governance requirements.\n\nCoding Style: Follow AWS SDK best practices, implement proper error handling and retries, use async programming for API calls, maintain comprehensive logging, follow AWS security best practices, implement rate limiting for API calls.",
    contextTracking: "Use DynamoDB to track assessment history, improvement progress, compliance scores, recommendation implementation status, and architectural maturity across all AWS accounts and workloads. Generate trend analysis and improvement reports.",
    relationalContext: "You are an AWS Solutions Architect with deep expertise in the Well-Architected Framework, enterprise cloud optimization, and AWS best practices. You understand architectural patterns, optimization strategies, and enterprise cloud governance.",
    specificTask: "Design and implement a comprehensive AWS Well-Architected assessment platform that automatically evaluates cloud architectures and provides prioritized optimization recommendations across all five pillars.",
    guidelines: "Follow these AWS Well-Architected principles:\n- Design for failure and implement reliability patterns\n- Apply security in depth with defense strategies\n- Optimize performance through right-sizing and caching\n- Implement cost optimization through resource efficiency\n- Enable operational excellence through automation\n- Consider sustainability in architectural decisions",
    taskConstraints: "The assessment platform must:\n- Evaluate all five Well-Architected pillars\n- Provide actionable recommendations\n- Support large-scale AWS environments\n- Generate comprehensive reports\n- Enable continuous improvement tracking",
    finopsConsiderations: "Cost Estimation: Calculate platform operational costs including AWS API calls, Lambda executions, and storage requirements. Cost Drivers: Primary costs from automated assessments, data collection, and recommendation engines. Right-Sizing: Optimize assessment frequency and resource allocation for cost efficiency. Cost Monitoring: Track platform costs and measure ROI through identified optimization opportunities. Budget Allocation: Ensure platform investment generates significant cost savings through architectural improvements and optimization recommendations."
  },
  {
    projectRequirements: "Project Overview: Developing a comprehensive Azure networking automation platform that provides software-defined networking, security policy enforcement, and network optimization across global Azure deployments. The platform simplifies complex networking configurations while maintaining security and performance.\n\nTech Stack: Azure PowerShell, Azure CLI, Azure Resource Manager, Azure Virtual Network, Azure Firewall, Azure Application Gateway, Azure Traffic Manager, Azure Network Watcher, Azure Monitor, PowerShell DSC, Terraform for infrastructure.\n\nKey Requirements: Automated network provisioning, security group management, traffic routing optimization, network monitoring and analytics, connectivity troubleshooting, bandwidth optimization, disaster recovery networking, and integration with on-premises networks.\n\nConstraints: Support for hybrid cloud connectivity, ensure network security compliance, maintain high availability across regions, handle complex routing requirements, support network segmentation, comply with data sovereignty requirements.\n\nCoding Style: Follow Azure networking best practices, use consistent naming conventions, implement proper error handling, maintain comprehensive documentation, use Infrastructure as Code, follow security-first design principles.",
    contextTracking: "Use Azure Monitor and Network Watcher to track network performance metrics, security events, traffic patterns, connectivity status, and optimization opportunities across all Azure networking components. Generate network health and security reports.",
    relationalContext: "You are an Azure network architect with expertise in software-defined networking, Azure networking services, and enterprise network security. You understand network protocols, security frameworks, and cloud networking patterns.",
    specificTask: "Design and implement a comprehensive Azure networking automation platform that provides secure, scalable, and optimized network infrastructure with automated management and monitoring capabilities.",
    guidelines: "Follow these Azure networking best practices:\n- Implement network segmentation with NSGs\n- Use Azure Firewall for centralized security\n- Apply hub-and-spoke topology patterns\n- Implement proper DNS configuration\n- Use private endpoints for secure connectivity\n- Enable network monitoring and analytics",
    taskConstraints: "The networking platform must:\n- Support global Azure deployments\n- Provide automated security enforcement\n- Enable hybrid cloud connectivity\n- Offer network performance optimization\n- Support disaster recovery scenarios",
    finopsConsiderations: "Cost Estimation: Calculate monthly costs for Azure networking services including virtual networks, firewalls, gateways, and data transfer. Cost Drivers: Primary costs from network gateway usage, firewall processing, and inter-region data transfer. Right-Sizing: Optimize gateway SKUs and implement efficient routing to minimize costs. Cost Monitoring: Track networking costs and implement data transfer optimization strategies. Budget Allocation: Balance networking performance and security requirements with cost optimization through intelligent routing and resource sizing."
  },
  {
    projectRequirements: "Project Overview: Building an advanced Linux performance optimization and monitoring platform that provides real-time system analysis, automated tuning recommendations, and proactive issue resolution across large-scale Linux deployments. The platform ensures optimal system performance and reliability.\n\nTech Stack: Linux kernel tools (perf, ftrace, eBPF), Prometheus for metrics collection, Grafana for visualization, Node Exporter, Custom Python scripts, Ansible for automation, SystemTap for advanced profiling, NUMA topology tools, cgroups for resource management.\n\nKey Requirements: Real-time performance monitoring, automated performance tuning, resource utilization optimization, bottleneck identification, capacity planning, kernel parameter optimization, application performance profiling, and integration with existing monitoring infrastructure.\n\nConstraints: Support for multiple Linux distributions, maintain low monitoring overhead, provide actionable performance insights, handle high-frequency data collection, support enterprise-scale deployments, ensure monitoring reliability and accuracy.\n\nCoding Style: Follow Linux system programming best practices, use efficient data collection methods, implement proper error handling, maintain minimal system impact, use appropriate kernel interfaces, follow security hardening guidelines.",
    contextTracking: "Use a time-series database to track system performance metrics, optimization actions, performance improvements, resource utilization trends, and system health indicators across all monitored Linux systems. Generate performance optimization reports.",
    relationalContext: "You are a Linux performance engineer with deep expertise in kernel internals, system optimization, and enterprise infrastructure monitoring. You understand Linux performance tools, kernel tuning, and system architecture optimization.",
    specificTask: "Design and implement an advanced Linux performance optimization platform that provides real-time monitoring, automated tuning recommendations, and proactive performance management for enterprise Linux environments.",
    guidelines: "Follow these Linux performance optimization practices:\n- Use appropriate performance monitoring tools\n- Implement CPU, memory, and I/O optimization\n- Apply kernel parameter tuning strategies\n- Use cgroups for resource management\n- Implement NUMA-aware optimizations\n- Enable proactive performance monitoring",
    taskConstraints: "The optimization platform must:\n- Monitor systems with minimal overhead\n- Provide real-time performance insights\n- Generate automated tuning recommendations\n- Support large-scale deployments\n- Enable proactive issue resolution",
    finopsConsiderations: "Cost Estimation: Calculate monthly costs for monitoring infrastructure, performance tooling, and optimization automation across Linux fleet. Cost Drivers: Primary costs from monitoring storage, analysis compute resources, and automation tools. Right-Sizing: Optimize monitoring frequency and data retention policies for cost efficiency. Cost Monitoring: Track platform operational costs and measure ROI through performance improvements and reduced downtime. Budget Allocation: Balance monitoring investment with operational efficiency gains and system reliability improvements."
  }
];
